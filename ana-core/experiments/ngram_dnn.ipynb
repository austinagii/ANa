{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram BoW Deep Neural Network Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Objective\n",
    "--\n",
    "Create a neural network to predict the next token in a sequence using the bag of words representation of the current ngram\n",
    "\n",
    "Limitations\n",
    "--\n",
    "1. Character level tokenization will be used\n",
    "2. Bag of words representation of an ngram will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @tiffanylue i know  i was listenin to bad habi...\n",
       "1    Layin n bed with a headache  ughhhh...waitin o...\n",
       "2                  Funeral ceremony...gloomy friday...\n",
       "3                 wants to hang out with friends SOON!\n",
       "4    @dannycastillo We want to trade with someone w...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/text/text_emotion.csv')['content']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "END_TOKEN = \"<E>\"\n",
    "\n",
    "# Build the vocabulary for this training set.\n",
    "vocab = [END_TOKEN] + list(set(dataset.str.cat(sep=' ')))\n",
    "vocab_size = len(vocab)\n",
    "# Assign a unique id to each token in the vocabulary.\n",
    "id_by_token = {token: i for (i, token) in enumerate(vocab)}\n",
    "token_by_id = {id_: token for (token, id_) in id_by_token.items()}\n",
    "\n",
    "class Codec:\n",
    "    @staticmethod\n",
    "    def encode(token: str) -> int:\n",
    "        return id_by_token[token]\n",
    "\n",
    "    @staticmethod    \n",
    "    def encode_all(tokens: list[str]) -> int:\n",
    "        return [Codec.encode(token) for token in tokens]\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(encoded_token: int) -> str:\n",
    "        return token_by_id[encoded_token]\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode_all(encoded_tokens: list[str]) -> int:\n",
    "        return [Codec.decode(encoded_token) for encoded_token in encoded_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    @staticmethod\n",
    "    def tokenize(document: str, is_complete: str = True) -> list[str]:\n",
    "        tokens = [END_TOKEN] + list(document) \n",
    "        if is_complete:\n",
    "            tokens.append(END_TOKEN)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_dataset = dataset[: math.floor(dataset_size * train_size)]\n",
    "test_dataset = dataset[math.floor(dataset_size * train_size):]\n",
    "test_dataset.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_document(dataset, min_tokens = None) -> tuple[list, list[str]]:\n",
    "    \"\"\"Returns a random document from the specified dataset, containing at least `min_tokens` tokens\"\"\"\n",
    "    # Select documents at random until one with the minimum number of tokens is found.\n",
    "    while True:\n",
    "        document = dataset[random.randrange(0, dataset.shape[0])]\n",
    "        if min_tokens is not None and len(Tokenizer.tokenize(document)) < min_tokens:\n",
    "            continue\n",
    "        break\n",
    "    return document\n",
    "\n",
    "def sample_tokens(document: str, num_tokens: int) -> str:\n",
    "    \"\"\"Returns a random sub-sequence of tokens from the specified document\"\"\"\n",
    "    tokens = Tokenizer.tokenize(document)\n",
    "    if len(tokens) < num_tokens:\n",
    "        raise ValueError(\"The provided document does not contain enough tokens to return the requested amount\") \n",
    "    start = random.randrange(0, len(tokens) - num_tokens + 1)\n",
    "    return tokens[start: start + num_tokens]\n",
    "\n",
    "def to_ngrams(tokens: list[str], ngram_size: int) -> list[list[str]]:\n",
    "    \"\"\"Returns ngrams from the specified document\"\"\"\n",
    "    return [tokens[max(0, i - ngram_size): i] for i in range(1, len(tokens) + 1)]\n",
    "\n",
    "def to_context_token_pairs(tokens: list[str], ngram_size: int) -> list[tuple[list[str], str]]:\n",
    "    \"\"\"Converts a token sequence to a sequence of context-token pairs.\n",
    "\n",
    "    Each 'context-token' pair is a tuple containing a 'context', which is an n-gram of \n",
    "    length [1: `ngram_size`], and a 'token' that immediately follows that n-gram in the \n",
    "    sequence.\n",
    "\n",
    "    Pairs are generated by iterating over the tokens in the range [1, `len(tokens)`]. The token \n",
    "    at the current index becomes the 'token' in the pair, while the preceding `ngram_size` \n",
    "    tokens serve as the 'context'.\n",
    "    \"\"\"\n",
    "    ngrams = to_ngrams(tokens, ngram_size=ngram_size)\n",
    "    ngrams = ngrams[0: len(ngrams) - 1]  # Truncate the last ngram since this wont have a trailing token \n",
    "                                         # and therefore cant form a pair.\n",
    "    return [(ngrams[i], tokens[i+1]) for i in range(len(ngrams))]\n",
    "\n",
    "def encode_context_token_pairs(pairs):\n",
    "    encoded_contexts = torch.zeros(len(pairs), vocab_size)\n",
    "    encoded_tokens = torch.zeros(len(pairs), dtype=torch.long)\n",
    "    for i, (context, token) in enumerate(pairs):\n",
    "        for t in context:\n",
    "            encoded_contexts[i, Codec.encode(t)] += 1\n",
    "        encoded_tokens[i] = Codec.encode(token)\n",
    "    return encoded_contexts, encoded_tokens\n",
    "\n",
    "def sample_context_token_pairs(dataset: pd.Series, num_pairs: int, ngram_size: int):\n",
    "    \"\"\"Returns a random sequence of context-token pairs from the specified dataset\"\"\"\n",
    "    num_tokens_required = num_pairs + 1 # n context-token pairs requires n + 1 tokens\n",
    "    document = get_random_document(dataset, min_tokens=num_tokens_required)\n",
    "    tokens = sample_tokens(document, num_tokens=num_tokens_required)\n",
    "    return to_context_token_pairs(tokens, ngram_size=ngram_size)\n",
    "\n",
    "def sample_batch(dataset, batch_size, ngram_size, device=None):\n",
    "    pairs = sample_context_token_pairs(dataset, num_pairs=batch_size, ngram_size=ngram_size)\n",
    "    X, y = encode_context_token_pairs(pairs)\n",
    "    if device is not None:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "    return X, y\n",
    "\n",
    "def sample_batch_v2(dataset, batch_size, ngram_size, device=None):\n",
    "    document = get_random_document(train_dataset, min_tokens=batch_size+1)\n",
    "    tokens = Tokenizer.tokenize(document)\n",
    "    context_token_pairs = to_context_token_pairs(tokens, ngram_size=ngram_size)\n",
    "    start = random.randint(0, len(context_token_pairs) - batch_size + 1)\n",
    "    pairs = context_token_pairs[start: start + batch_size]\n",
    "    X, y = encode_context_token_pairs(pairs)\n",
    "    if device is not None:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we will use a simple neural network, 5 layers deep as a test. Once the learning ability of this model is proven we can evaulate using a larger neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(vocab_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ngram_size = 3\n",
    "batch_size = 64\n",
    "num_train_batches = 20_000\n",
    "num_test_batches = 6000\n",
    "num_epochs = 64\n",
    "print_interval = 5_000\n",
    "\n",
    "model = Model().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Epoch #1\n",
      "  Train batch #5000 -- loss: 3.147500 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 3.357711 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 3.388016 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.940302 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 21.369271%, Avg loss[3.023184333920479]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #2\n",
      "  Train batch #5000 -- loss: 3.052405 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 3.120704 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 3.017201 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.909417 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 25.079948%, Avg loss[2.8625915988286335]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #3\n",
      "  Train batch #5000 -- loss: 3.174700 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 3.110636 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.641510 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.495705 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 26.475000%, Avg loss[2.7996006519794463]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #4\n",
      "  Train batch #5000 -- loss: 2.894788 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.860735 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.718106 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.855026 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 27.259635%, Avg loss[2.7412631282011666]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #5\n",
      "  Train batch #5000 -- loss: 2.432905 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.847931 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.691489 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.476564 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 28.783333%, Avg loss[2.6980631159345307]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #6\n",
      "  Train batch #5000 -- loss: 2.476784 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.375655 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.692258 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.540479 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 29.305729%, Avg loss[2.662327472617229]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #7\n",
      "  Train batch #5000 -- loss: 2.892203 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 3.208836 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.750036 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 3.082696 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 30.628646%, Avg loss[2.624801951944828]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #8\n",
      "  Train batch #5000 -- loss: 2.629109 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.805273 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.971306 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.693120 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 31.231510%, Avg loss[2.589541930953662]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #9\n",
      "  Train batch #5000 -- loss: 3.293199 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.624244 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.273522 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.364448 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 31.280208%, Avg loss[2.590766824195782]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #10\n",
      "  Train batch #5000 -- loss: 2.818305 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.455842 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.369075 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.282722 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 32.408594%, Avg loss[2.5443438455661136]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #11\n",
      "  Train batch #5000 -- loss: 2.303742 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.257643 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.544178 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.595608 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 33.064583%, Avg loss[2.516844806621472]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #12\n",
      "  Train batch #5000 -- loss: 2.298886 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.474453 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.357950 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.321456 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 33.238542%, Avg loss[2.507413401931524]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #13\n",
      "  Train batch #5000 -- loss: 2.945103 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.987363 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.526812 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.055946 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 33.504948%, Avg loss[2.4905882980624834]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #14\n",
      "  Train batch #5000 -- loss: 2.635336 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.785614 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.771125 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.262219 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 33.775781%, Avg loss[2.4778513711492223]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #15\n",
      "  Train batch #5000 -- loss: 2.466723 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.366380 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.366425 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.335654 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.167448%, Avg loss[2.462444621553024]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #16\n",
      "  Train batch #5000 -- loss: 2.961502 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.892341 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.708128 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.217323 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.107813%, Avg loss[2.4625632461706797]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #17\n",
      "  Train batch #5000 -- loss: 2.725853 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.738764 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.570562 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.801373 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.565104%, Avg loss[2.4438795157670974]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #18\n",
      "  Train batch #5000 -- loss: 3.191180 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.561209 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.815566 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.177376 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.339844%, Avg loss[2.4501336409052215]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #19\n",
      "  Train batch #5000 -- loss: 2.345583 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.379900 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.550658 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.662995 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.139323%, Avg loss[2.4504752737283706]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #20\n",
      "  Train batch #5000 -- loss: 2.472927 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.468298 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.543727 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.350662 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.585156%, Avg loss[2.436866539676984]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #21\n",
      "  Train batch #5000 -- loss: 2.479258 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.171624 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 1.955980 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.396851 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.820573%, Avg loss[2.4172189782857894]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #22\n",
      "  Train batch #5000 -- loss: 2.715210 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.929050 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.446917 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.366175 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.706250%, Avg loss[2.4222728336552777]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #23\n",
      "  Train batch #5000 -- loss: 2.369951 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.266467 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.153152 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.274324 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.679167%, Avg loss[2.4150802602767945]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #24\n",
      "  Train batch #5000 -- loss: 2.003677 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.110502 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 1.906645 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 1.985172 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.663802%, Avg loss[2.4205586939056714]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #25\n",
      "  Train batch #5000 -- loss: 2.393323 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.599811 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.170078 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.161674 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 34.508333%, Avg loss[2.4233709231615066]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #26\n",
      "  Train batch #5000 -- loss: 2.230255 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.436389 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.655451 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.734471 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.173958%, Avg loss[2.4026970955530804]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #27\n",
      "  Train batch #5000 -- loss: 2.472969 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.629050 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.144085 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.471500 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.017708%, Avg loss[2.4045953032771745]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #28\n",
      "  Train batch #5000 -- loss: 2.234750 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.094621 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.738085 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.060742 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.033854%, Avg loss[2.402473371783892]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #29\n",
      "  Train batch #5000 -- loss: 2.207962 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.465520 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.394449 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.337112 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.023958%, Avg loss[2.3948879287640255]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #30\n",
      "  Train batch #5000 -- loss: 2.036400 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.190177 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 1.771581 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.199646 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.229688%, Avg loss[2.3902885205944377]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #31\n",
      "  Train batch #5000 -- loss: 2.233503 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.020756 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.240184 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.763983 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.009896%, Avg loss[2.3906644066770872]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #32\n",
      "  Train batch #5000 -- loss: 2.704210 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.094954 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.319035 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.871641 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.137240%, Avg loss[2.388728331973155]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #33\n",
      "  Train batch #5000 -- loss: 2.305005 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.273358 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.322584 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.119810 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.364062%, Avg loss[2.3803294376333555]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #34\n",
      "  Train batch #5000 -- loss: 2.272964 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.483688 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.227779 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.303921 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.316667%, Avg loss[2.38458283029]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #35\n",
      "  Train batch #5000 -- loss: 2.403853 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.437101 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.546453 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 3.012014 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.166927%, Avg loss[2.3897231987913448]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #36\n",
      "  Train batch #5000 -- loss: 2.408422 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 1.853534 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.454408 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.385950 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.227604%, Avg loss[2.3765284415483476]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #37\n",
      "  Train batch #5000 -- loss: 2.428051 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 1.919994 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.496052 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.614769 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.571354%, Avg loss[2.371642757674058]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #38\n",
      "  Train batch #5000 -- loss: 2.983969 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.401237 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.199324 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.199634 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.283073%, Avg loss[2.3808654487133025]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #39\n",
      "  Train batch #5000 -- loss: 1.955960 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 3.038486 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.869284 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.135907 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.706510%, Avg loss[2.3695472985804082]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #40\n",
      "  Train batch #5000 -- loss: 2.201967 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 1.965909 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.296277 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.681345 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.709115%, Avg loss[2.367215396980445]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #41\n",
      "  Train batch #5000 -- loss: 1.931407 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.253803 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.150855 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.075712 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.417448%, Avg loss[2.3765433859626452]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #42\n",
      "  Train batch #5000 -- loss: 2.052061 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.199922 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 3.004297 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.031436 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.763281%, Avg loss[2.367859322309494]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #43\n",
      "  Train batch #5000 -- loss: 2.714842 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.586480 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.011706 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.613579 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.575521%, Avg loss[2.3689417863984903]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #44\n",
      "  Train batch #5000 -- loss: 2.889360 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.610481 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 1.998218 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.189878 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.679948%, Avg loss[2.3750139132936794]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #45\n",
      "  Train batch #5000 -- loss: 2.561923 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 1.917772 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.309656 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.889767 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.505208%, Avg loss[2.3618972007433574]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #46\n",
      "  Train batch #5000 -- loss: 2.182687 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 1.892421 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.234943 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 1.970603 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.901042%, Avg loss[2.359395056227843]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #47\n",
      "  Train batch #5000 -- loss: 1.952500 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.252203 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.361166 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 1.997754 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.857813%, Avg loss[2.354269298414389]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #48\n",
      "  Train batch #5000 -- loss: 2.043682 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.129261 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.531557 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.374610 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.226042%, Avg loss[2.3693575415611265]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #49\n",
      "  Train batch #5000 -- loss: 2.078886 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.010474 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.418584 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.570464 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.599479%, Avg loss[2.361706973095735]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #50\n",
      "  Train batch #5000 -- loss: 2.595788 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.628126 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.231969 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.612178 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.542708%, Avg loss[2.366052284200986]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #51\n",
      "  Train batch #5000 -- loss: 2.573094 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.252833 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.786894 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.022581 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.680208%, Avg loss[2.358645675698916]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #52\n",
      "  Train batch #5000 -- loss: 2.602384 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.339214 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.888687 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.682568 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.675781%, Avg loss[2.3536458385984105]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #53\n",
      "  Train batch #5000 -- loss: 1.948210 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.277175 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.556010 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.031877 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.720573%, Avg loss[2.3552985940178237]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #54\n",
      "  Train batch #5000 -- loss: 2.322551 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.276765 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.070658 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.130022 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.657031%, Avg loss[2.3532445050676665]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #55\n",
      "  Train batch #5000 -- loss: 2.270442 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.085519 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.404601 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.412415 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.879948%, Avg loss[2.3440068590243657]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #56\n",
      "  Train batch #5000 -- loss: 2.165653 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 1.788099 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.746471 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.489421 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.869010%, Avg loss[2.3512487654089926]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #57\n",
      "  Train batch #5000 -- loss: 2.500199 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.128175 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.308049 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.590544 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.755990%, Avg loss[2.347132700661818]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #58\n",
      "  Train batch #5000 -- loss: 2.272552 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 1.892794 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.273249 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 3.027739 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.716667%, Avg loss[2.3527476374109586]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #59\n",
      "  Train batch #5000 -- loss: 2.007164 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.249809 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.281587 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.472563 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 36.018229%, Avg loss[2.3473891535600027]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #60\n",
      "  Train batch #5000 -- loss: 3.143030 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.585855 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 1.834127 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 1.912199 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.877865%, Avg loss[2.3423308546940484]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #61\n",
      "  Train batch #5000 -- loss: 2.215606 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.648774 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.029556 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.635521 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.873177%, Avg loss[2.3483868758678437]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #62\n",
      "  Train batch #5000 -- loss: 2.164641 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.177225 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.372270 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.196579 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.829427%, Avg loss[2.3545907941063247]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #63\n",
      "  Train batch #5000 -- loss: 2.257288 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.008627 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.005611 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 2.292327 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.733854%, Avg loss[2.34692519758145]\n",
      "\n",
      "-------------------------------------------\n",
      "Epoch #64\n",
      "  Train batch #5000 -- loss: 2.458393 [5000 / 20000]\n",
      "  Train batch #10000 -- loss: 2.330232 [10000 / 20000]\n",
      "  Train batch #15000 -- loss: 2.711372 [15000 / 20000]\n",
      "  Train batch #20000 -- loss: 1.996327 [20000 / 20000]\n",
      "  Evaluation -- Test Accuracy: 35.769531%, Avg loss[2.3520286161700885]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_no in range(1, num_epochs+1):\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"Epoch #{epoch_no}\")\n",
    "    # train \n",
    "    for batch_no in range(1, num_train_batches+1):\n",
    "        X, y = sample_batch_v2(train_dataset, batch_size=batch_size, ngram_size=ngram_size, device=device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch_no % print_interval == 0:\n",
    "            print(f\"  Train batch #{batch_no} -- loss: {loss:>7f} [{batch_no} / {num_train_batches}]\")\n",
    "    \n",
    "    # evaluate\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    for batch_no in range(num_test_batches):\n",
    "        X, y = sample_batch_v2(test_dataset, batch_size=batch_size, ngram_size=ngram_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y).item()\n",
    "            correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    print(f\"  Evaluation -- Test Accuracy: {((correct / (num_test_batches * batch_size)) * 100):>3f}%, Avg loss[{loss / num_test_batches}]\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_bag_of_words(encoded_tokens, size: int) -> torch.Tensor:\n",
    "    bow = torch.zeros(size)\n",
    "    for encoded_token in encoded_tokens:\n",
    "        bow[encoded_token] += 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt: str = None):\n",
    "    if prompt is None:\n",
    "        prompt = Codec.decode(random.randint(0, vocab_size))\n",
    "        \n",
    "    while len(prompt) < 20:\n",
    "        tokens = Tokenizer.tokenize(prompt, is_complete=False)\n",
    "        ngrams = to_ngrams(tokens, ngram_size=3)\n",
    "        last_ngram = ngrams[len(ngrams) - 1]\n",
    "        with torch.no_grad():\n",
    "            # print(f\"Current ngram: {last_ngram}\")\n",
    "            # print(f\"Current ngram as bow: {to_bag_of_words(Codec.encode_all(last_ngram), size=vocab_size)}\")\n",
    "            encoded_ngram = to_bag_of_words(Codec.encode_all(last_ngram), size=vocab_size).reshape(1, -1).to(device)\n",
    "            # print(encoded_ngram)\n",
    "            # break\n",
    "            logits = model(encoded_ngram)\n",
    "            proba = nn.Softmax(dim=1)(logits)[0]\n",
    "            top_predictions = proba.topk(5).indices.tolist()\n",
    "            # print(top_predictions)\n",
    "            # break\n",
    "            # print(\"Top 5 most likely next tokens:\")\n",
    "            # for i, encoding in enumerate(top_predictions):\n",
    "            #     print(f'    #{i}. \"{Codec.decode(encoding)}\"')\n",
    "            # break\n",
    "            next_token = Codec.decode(torch.multinomial(proba, num_samples=1)[0].item())\n",
    "            # print(f'\"{next_token}\" was chosen')\n",
    "            prompt += next_token\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: SeA  just etgt os is\n",
      "#2: bestr hady oughs. Wo\n",
      "#3: {Tos  lafsoe nICas. \n",
      "#4: jutale asI amknay! 2\n",
      "#5: 0mfwed saIdd oisng m\n",
      "#6: lsee shtpe yofollml \n",
      "#7: )I'm  istley any oua\n",
      "#8: Jyuit  o fro mtyy os\n",
      "#9: PL@EOV HE I  rlhile \n",
      "#10: 0.@ borat thatteeh a\n",
      "#11: dje untnree nse hvbi\n",
      "#12: zthi mtahtsogte out \n",
      "#13: GO *OLT GOD Ladrs mn\n",
      "#14: ½geols. sIe  tsos mo\n",
      "#15: 5 un tmhe wdie b oyu\n",
      "#16: bfeor my si dti scho\n",
      "#17: zeon riged  Junate o\n",
      "#18: on ein gGoerdse yabe\n",
      "#19: Y_uya nsei n Beer ha\n",
      "#20: Rry on methe flseena\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(f\"#{i+1}: {generate_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible reasons for poor performance:\n",
    "\n",
    "#### Loss of ordering due to bag of words representation\n",
    "The current implementation takes an ngram, an inherently ordered structure and destroy the order by representing it as a bag of words / bag of tokens. The algorithm them is expected to \n",
    "\n",
    "#### Sparsity of the input data\n",
    "Input is the size of the vocabulary which in our case is ~100 characters. Since the ngram size used for experiment was often less than 5, at any given time only 5 of ~100 input values would be non-zero. This could have made learning difficult (investigate this)\n",
    "\n",
    "#### Type of neural network used / Depth & Shape of the neural network\n",
    "The neural network chosen does not have an inherrent notion of ordering and works with data at a specific point in time. This combined with the loss of ordering information in the input may result in poor performance. There is also an admitted lack of experience with regards to optimizing the shape of a neural network for next character prediction. There may be a way to optimize just a regular MLP for this task but the experience is lacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "What can I learn coming out of this?\n",
    "\n",
    "1. Why does sparsity affect the learning ability of a neural network?\n",
    "2. How does learning rate affect the learning ability of neural network? What happens when learning rate values become too large or small?\n",
    "3. How does a neural network learn in the first place? Can you learn to guestimate how well a given neural network will do for a specific dataset / task\n",
    "4. How does cross entropy loss work? Why does the formula use the exponent rather than simple absolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
