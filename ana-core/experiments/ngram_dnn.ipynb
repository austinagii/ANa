{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Bag of Tokens Neural Network\n",
    "\n",
    "This experiment seeks to create a neural network to predict the next token in a sequence, given a bag of tokens representation of the preceeding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author   \n",
       "0  1956967341       empty   xoshayzers  \\\n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset := pd.read_csv('../data/text/text_emotion.csv')).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the dataset to lowercase.\n",
    "dataset = dataset['content'].str.lower()\n",
    "# Split the dataset into train and test sets.\n",
    "train_dataset, test_dataset = dataset[: (train_dataset_size := int(dataset.shape[0] * train_size))], dataset[train_dataset_size:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    end_token = \"<E>\"\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(document: str, is_complete: str = True) -> list[str]:\n",
    "        tokens = [Tokenizer.end_token] + list(document) \n",
    "        if is_complete and tokens[len(tokens) - 1] != Tokenizer.end_token:\n",
    "            tokens.append(Tokenizer.end_token)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Codec:\n",
    "    end_token = \"<E>\"\n",
    "    \n",
    "    def __init__(self, documents: Iterable[str]): \n",
    "        self.vocab = set(Tokenizer.end_token)\n",
    "        for document in documents:\n",
    "            for token in Tokenizer.tokenize(document):\n",
    "                self.vocab.add(token)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.encoding_by_token = {token: encoding for encoding, token in enumerate(self.vocab)}\n",
    "        self.token_by_encoding = {encoding: token for token, encoding in self.encoding_by_token.items()} \n",
    "\n",
    "    def encode(self, tokens: Union[str, list[str], tuple[str, ...]]) -> Union[str, list[str], tuple[str, ...]]:\n",
    "        if isinstance(tokens, str):\n",
    "            encodings = self.encoding_by_token[tokens]\n",
    "        elif (isinstance(tokens, list) or isinstance(tokens, tuple)) and all(isinstance(token, str) for token in tokens):\n",
    "            encodings = type(tokens)((self.encoding_by_token[token] for token in tokens))\n",
    "        return encodings\n",
    "\n",
    "    def decode(self, encodings: Union[int, list[int], tuple[int, ...]]) -> Union[int, list[int], tuple[int, ...]]:\n",
    "        if isinstance(encodings, int):\n",
    "            tokens = self.token_by_encoding[encodings]\n",
    "        elif (isinstance(encodings, list) or isinstance(encodings, tuple)) and all(isinstance(encoding, int) for encoding in encodings):\n",
    "            tokens = type(encodings)((self.token_by_encoding[encoding] for encoding in encodings))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextTokenPair:\n",
    "    def __init__(self, context, token, document_index=None, context_start_index=None, context_size=None):\n",
    "        self.context = context\n",
    "        self.token = token\n",
    "        self.document_index = document_index\n",
    "        self.context_index = (context_start_index, context_start_index + context_size)\n",
    "        self.token_index = context_start_index + context_size\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ContextTokenPair(context={}, token='{}', document_index={}, context_index={}, token_index={})\".format(\n",
    "            self.context, self.token, self.document_index, self.context_index, self.token_index\n",
    "        )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "\n",
    "def to_context_token_pairs(documents: Iterable[str], context_size: int) -> Iterable[ContextTokenPair]:\n",
    "    \"\"\"Generates context-token pairs from the given documents.\n",
    "    \n",
    "    Context-token pairs are generated in the order they appear in the documents with context length fixed to the given context size.\n",
    "    \"\"\"\n",
    "    for document_index, document in enumerate(documents):\n",
    "        tokens = Tokenizer.tokenize(document)\n",
    "        # Skip documents lacking sufficient tokens for a context-token pair with the required size.\n",
    "        if (total_tokens := len(tokens)) < context_size + 1:\n",
    "            continue\n",
    "        for context_start_index in range(total_tokens - context_size):\n",
    "            context = tokens[context_start_index : context_start_index + context_size]\n",
    "            token = tokens[context_start_index + context_size]\n",
    "            yield ContextTokenPair(context, token, document_index=document_index, context_start_index=context_start_index, context_size=context_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingBatch:\n",
    "    def __init__(self, examples: Iterable[ContextTokenPair]):\n",
    "        self.examples = examples\n",
    "        self.batch_size = len(examples)\n",
    "\n",
    "    def encode(self, codec: Codec):\n",
    "        X, y = torch.zeros((self.batch_size, codec.vocab_size)), torch.zeros((self.batch_size, codec.vocab_size))\n",
    "        for i, example in enumerate(self.examples):\n",
    "            for encoding in codec.encode(example.context):\n",
    "                X[i][encoding] += 1\n",
    "            y[i][codec.encode(example.token)] = 1\n",
    "        return X, y\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"TrainingBatch(batch_size={}, batch=[{}])\".format(\n",
    "            self.batch_size, \"\\n\" + \"\".join([\"  \" + example.__str__() + \"\\n\" for example in self.examples])\n",
    "        )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "\n",
    "def to_training_batches(training_examples: Iterable[ContextTokenPair], batch_size: int) -> Iterable[TrainingBatch]:\n",
    "    batch = [None] * batch_size\n",
    "    for i, training_example in enumerate(training_examples):\n",
    "        batch[(curr_index := i % batch_size)] = training_example\n",
    "        if curr_index == batch_size - 1:\n",
    "            yield TrainingBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_training_example(dataset, context_size) -> ContextTokenPair:\n",
    "    \"\"\"Returns a random training example from the dataset.\"\"\"\n",
    "    # Select documents at random until one with the minimum number of tokens is found.\n",
    "    while True:\n",
    "        document = dataset[(document_index := random.randrange(0, dataset.shape[0]))]\n",
    "        if len(tokens := Tokenizer.tokenize(document)) >= context_size + 1:\n",
    "            break\n",
    "    \n",
    "    # Select a random context from the document.\n",
    "    context_index = random.randrange(0, len(tokens) - context_size)\n",
    "    token_index = context_index + context_size\n",
    "    context, token = tokens[context_index: token_index], tokens[token_index]\n",
    "    return ContextTokenPair(context, token, document_index, context_index, token_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "codec = Codec(train_dataset)\n",
    "training_examples = to_context_token_pairs(train_dataset, context_size=7)\n",
    "training_batches = to_training_batches(training_examples, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we will use a simple neural network, 5 layers deep as a test. Once the learning ability of this model is proven we can evaulate using a larger neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(codec.vocab_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(32, 32),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(32, 32),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(32, 32),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(32, 32),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, codec.vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ngram_size = 3\n",
    "batch_size = 16\n",
    "num_train_batches = 40_000\n",
    "num_test_batches = 10_000\n",
    "num_epochs = 128\n",
    "print_interval = 5_000\n",
    "\n",
    "model = Model().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch_no in range(1, num_epochs+1):\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"Epoch #{epoch_no}\")\n",
    "    # train \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch_no in range(1, num_train_batches+1):\n",
    "        inputs, targets = sample_batch_v2(train_dataset, batch_size=batch_size, ngram_size=ngram_size, device=device)\n",
    "        logits = model(inputs)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        loss = loss_fn(logits, targets)\n",
    "        loss.backward(); optimizer.step(); optimizer.zero_grad()\n",
    "        total_loss += loss\n",
    "        correct = (predictions == targets).type(torch.float).sum().item()\n",
    "        total_correct += correct\n",
    "        # print the batch and average stats\n",
    "        if batch_no % print_interval == 0:\n",
    "            print(f\"  [{batch_no} / {num_train_batches}]: loss={(loss / batch_size):>7f}, Accuracy={((correct / batch_size) * 100):>3f}%, Avergage loss={(total_loss / (batch_size * batch_no)):>7f}, Accuracy={((total_correct / (batch_size * batch_no))* 100):>3f}%\")\n",
    "            # print(f\"  [{batch_no} / {num_train_batches}]: \")\n",
    "    \n",
    "    # evaluate\n",
    "    test_correct = 0\n",
    "    test_loss = 0\n",
    "    for batch_no in range(num_test_batches):\n",
    "        X, y = sample_batch_v2(test_dataset, batch_size=batch_size, ngram_size=ngram_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X)\n",
    "            test_loss += loss_fn(y_pred, y).item()\n",
    "            test_correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    print(f\"  Evaluation -- Test Accuracy: {((test_correct / (num_test_batches * batch_size)) * 100):>3f}%, Avg loss[{test_loss / (num_test_batches * batch_size)}]\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_next_token_proba(ngram, top_k=5):\n",
    "    ngram = Codec.encode_all(ngram)\n",
    "    with torch.no_grad():\n",
    "        encoded_ngram = to_bag_of_words(ngram, size=vocab_size).reshape(1, -1).to(device)\n",
    "        logits = model(encoded_ngram)\n",
    "        proba = nn.Softmax(dim=1)(logits)[0]\n",
    "        if top_k == -1:\n",
    "            top_k = proba.shape[0]\n",
    "        top_predictions = proba.topk(top_k).indices.tolist()\n",
    "    return [(f\"#{k+1}\", Codec.decode(prediction), proba[prediction]) for k, prediction in enumerate(top_predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_next_token(ngram, method='sample'):\n",
    "    ngram = Codec.encode_all(ngram)\n",
    "    with torch.no_grad():\n",
    "        encoded_ngram = to_bag_of_words(ngram, size=vocab_size).reshape(1, -1).to(device)\n",
    "        logits = model(encoded_ngram)\n",
    "        proba = nn.Softmax(dim=1)(logits)[0]\n",
    "        if method == 'sample':\n",
    "            next_token = torch.multinomial(proba, num_samples=1)[0].item()\n",
    "        elif method == 'top':\n",
    "            next_token = proba.topk(1).indices.tolist()[0]\n",
    "    return (next_token, Codec.decode(next_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_last_ngram(tokens: list[str], ngram_size: int) -> list[str]:\n",
    "    return tokens[max(0, len(tokens) - ngram_size): max(0, len(tokens) - ngram_size) + ngram_size]\n",
    "\n",
    "def to_bag_of_words(encoded_tokens, size: int) -> torch.Tensor:\n",
    "    bow = torch.zeros(size)\n",
    "    for encoded_token in encoded_tokens:\n",
    "        bow[encoded_token] += 1\n",
    "    return bow\n",
    "\n",
    "def get_random_token(encoded: bool = False):\n",
    "    random_token_id = random.randint(0, vocab_size)\n",
    "    if encoded:\n",
    "        return random_token_id\n",
    "    return Codec.decode(random_token_id)\n",
    "\n",
    "def complete_text(prompt: str = None, max_tokens = 80):\n",
    "    tokens = Codec.encode_all(Tokenizer.tokenize(prompt, is_complete=False))\n",
    "    is_text_size_limit_reached = lambda: len(tokens) >= max_tokens\n",
    "    is_last_token_end_token = lambda: tokens[len(tokens) - 1] == Codec.encode(END_TOKEN) and len(tokens) != 1\n",
    "    is_text_complete = lambda: is_text_size_limit_reached() or is_last_token_end_token()\n",
    "    while not is_text_complete():\n",
    "        last_ngram = get_last_ngram(tokens, ngram_size=3)\n",
    "        next_token, _ = predict_next_token(ngram, method='sample')\n",
    "        tokens.append(next_token)\n",
    "    return len(tokens), \"\".join(Codec.decode_all(tokens[1: -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complete_text('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_next_token_proba('g e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_similar_next_token_counts('g e').loc['g e'].sum_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis & Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why doesn't this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the dataset into it's constituent ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_ngrams = set()\n",
    "for document in train_dataset:\n",
    "    tokens = Tokenizer.tokenize(document)\n",
    "    for ngram in to_ngrams(tokens, ngram_size=ngram_size):\n",
    "        unique_ngrams.add(tuple(ngram))\n",
    "        \n",
    "print(f\"There are {vocab_size} unique tokens and {len(unique_ngrams)} unique ngrams in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_by_ngram = {ngram: idx for idx, ngram in enumerate(unique_ngrams)}\n",
    "ngram_by_id = {idx: ngram for ngram, idx in id_by_ngram.items()}\n",
    "\n",
    "class NgramCodec:\n",
    "    @staticmethod\n",
    "    def encode(ngram: str) -> int:\n",
    "        if isinstance(ngram, str) or isinstance(ngram, list):\n",
    "            ngram = tuple(ngram)\n",
    "        return id_by_ngram[ngram]\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(encoded_ngram: int) -> str:\n",
    "        return ngram_by_id[encoded_ngram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next_token_counts = np.zeros((len(unique_ngrams), vocab_size))\n",
    "for document in train_dataset:\n",
    "    tokens = Tokenizer.tokenize(document)\n",
    "    for context, token in to_context_token_pairs(tokens, ngram_size=ngram_size):\n",
    "        next_token_counts[NgramCodec.encode(context), Codec.encode(token)] += 1\n",
    "next_token_counts = pd.DataFrame(next_token_counts, index=[\"\".join(ngram) for ngram in ngram_by_id.values()], columns=list(token_by_id.values()))\n",
    "next_token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clone next token counts as the base for next token proba\n",
    "next_token_proba = torch.zeros((len(unique_ngrams), vocab_size))\n",
    "        \n",
    "next_token_proba = next_token_proba / next_token_proba.sum(dim=1, keepdim=True)\n",
    "next_token_proba_df = pd.DataFrame(next_token_proba.numpy(), \n",
    "                                   columns=list(token_by_id.values()),\n",
    "                                   index=[\"\".join(ngram) for idx, ngram in ngram_by_id.items()]) \n",
    "next_token_proba_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Let's Understand With An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see the model predictions for the trigram 'cab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "call_model(('c', 'a', 'b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the probabilities for each token aren't really that far apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our model accepts a bag of tokens, our model does not learn relationships of ordered tokens. Whether we pass 'cab' or 'bac' to the model all it will see are three 1s at position 23, 76 and 81.\n",
    "\n",
    "This means that all ngrams with combinations of these three characters will factor into the prediction about the next character, which will inherently produce some ambiguity in the prediction. i.e if the model sees 'cab' we would like to assume that 'i' may be prediction for a word like cabin, but considering that the model will see the same input for the ngram 'bac' in which case we may want 'k' to be the prediction to form back, the model will now have to balance both of these in the prediction based on their frequency of occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Codec.decode_all(call_model(['e', 'l', 'l']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'aab', 'baa' = 0\n",
    "'aab', 'aca' = 1  // {'a': 2, 'b': 1} {'a': 2, 'c': 1}\n",
    "'aab', 'acc' = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_similar(ngram, target):\n",
    "    return similarity(target, ngram) > 0\n",
    "\n",
    "def get_similar_ngrams(target, threshold=0):\n",
    "    similar_ngrams = [(s, n) for n in unique_ngrams if (s := similarity(target, n)) >= threshold] # get all similar ngrams with their similarity score\n",
    "    similar_ngrams.sort(key=lambda n: n[0], reverse=True) # sort the ngrams by their similarity score\n",
    "    return [ngram[1] for ngram in similar_ngrams] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_similar_next_token_counts(ngram, threshold=0):\n",
    "    similar_ngrams = get_similar_ngrams(ngram, threshold=threshold)\n",
    "    return next_token_counts.loc[[\"\".join(ngram) for ngram in similar_ngrams]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "call_model((' ', ' ', 'x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_similar_next_token_counts('  x', threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_similar_next_token_counts(('  '), threshold=0.6).sum().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next_token_proba_df.loc['  x'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_similar_next_token_counts(('x'))['<E>'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = Counter({'a': 1, 'b': 3})\n",
    "b = Counter({'b': 2, 'c': 1})\n",
    "What i want is the difference \n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def distance(from_, to_):\n",
    "    from_ = Counter(from_)\n",
    "    to_ = Counter(to_)\n",
    "    distance = 0\n",
    "    for token, _ in from_.items():\n",
    "        distance += abs(from_[token] - to_[token])\n",
    "    return distance\n",
    "\n",
    "def similarity(a, b):\n",
    "    \"\"\"Calculates the similarity between two ngrams\"\"\"\n",
    "    num_tokens = len(a)\n",
    "    d = distance(a, b)\n",
    "    return round((num_tokens - d) / num_tokens, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_similar_ngram_next_token_probas(target):\n",
    "    similar_ngrams = [(s, n) for n in unique_ngrams if (s := similarity((target), n)) > 0] # get all similar ngrams with their similarity score\n",
    "    similar_ngrams.sort(key=lambda n: n[0], reverse=True) # sort the ngrams by their similarity score\n",
    "    return next_token_proba_df.loc[[\"\".join(ngram[1]) for ngram in similar_ngrams]] # return the next token probas for similar ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_similar_ngram_next_token_probas(('c', 'a', 'r'))#.loc['<E>c\\''].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "find_documents_containing(train_dataset, \"c'm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_distance(from_, to):\n",
    "    counts = Counter(from_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_ngram_probas(target_ngram):\n",
    "    counts_by_token = Counter(ngram)\n",
    "    for ngram in unique_ngrams:\n",
    "        if distnace := get_distance(target_ngram, ngram) is not 0:\n",
    "            distances[distance].add(ngram):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "call_model(('car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[ngram for ngram in unique_ngrams if ('e' in ngram and 'l' in ngram)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_documents_containing(documents, substr):\n",
    "    matching_documents = []\n",
    "    for document in documents:\n",
    "        if substr in document:\n",
    "            matching_documents.append(document)\n",
    "    return matching_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "find_documents_containing(train_dataset, 'le8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Codec.decode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_proba[ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "to_ngrams(a, ngram_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.zeros(5,5).random_(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sum = a.sum(dim=1, keepdim=True)\n",
    "a_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a / a_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuples = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = ('a', 'b', 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuples.add(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = pred.argmax(dim=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = torch.empty(3, dtype=torch.long).random_(3)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.functional.cross_entropy(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 2., 5.])\n",
    "b = torch.tensor([3., 2., 5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(a == b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt: str = None, max_tokens=280):\n",
    "    if prompt is None:\n",
    "        encoded_tokens = [Codec.encode(END_TOKEN)]\n",
    "    \n",
    "    # TODO: Improve this condition.\n",
    "    while (len(encoded_tokens) == 1 or encoded_tokens[len(encoded_tokens) - 1] != Codec.encode(END_TOKEN)) and len(encoded_tokens) < max_tokens:\n",
    "        last_ngram = get_last_ngram(encoded_tokens, ngram_size=3)\n",
    "        with torch.no_grad():\n",
    "            # print(f\"Current ngram: {last_ngram}\")\n",
    "            # print(f\"Current ngram as bow: {to_bag_of_words(Codec.encode_all(last_ngram), size=vocab_size)}\")\n",
    "            encoded_ngram = to_bag_of_words(last_ngram, size=vocab_size).reshape(1, -1).to(device)\n",
    "            # print(encoded_ngram)\n",
    "            # break\n",
    "            logits = model(encoded_ngram)\n",
    "            proba = nn.Softmax(dim=1)(logits)[0]\n",
    "            top_predictions = proba.topk(5).indices.tolist()\n",
    "            # print(top_predictions)\n",
    "            # break\n",
    "            # print(\"Top 5 most likely next tokens:\")\n",
    "            # for i, encoding in enumerate(top_predictions):\n",
    "            #     print(f'    #{i}. \"{Codec.decode(encoding)}\"')\n",
    "            # break\n",
    "            next_token = torch.multinomial(proba, num_samples=1)[0].item()\n",
    "            # print(f'\"{Codec.decode(next_token)}\" was chosen')\n",
    "            encoded_tokens.append(next_token)\n",
    "    return len(encoded_tokens), \"\".join(Codec.decode_all(encoded_tokens[1: -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(f\"#{i+1}: {generate_text()[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible reasons for poor performance:\n",
    "\n",
    "#### Loss of ordering due to bag of words representation\n",
    "The current implementation takes an ngram, an inherently ordered structure and destroy the order by representing it as a bag of words / bag of tokens. The algorithm them is expected to \n",
    "\n",
    "#### Sparsity of the input data\n",
    "Input is the size of the vocabulary which in our case is ~100 characters. Since the ngram size used for experiment was often less than 5, at any given time only 5 of ~100 input values would be non-zero. This could have made learning difficult (investigate this)\n",
    "\n",
    "#### Type of neural network used / Depth & Shape of the neural network\n",
    "The neural network chosen does not have an inherrent notion of ordering and works with data at a specific point in time. This combined with the loss of ordering information in the input may result in poor performance. There is also an admitted lack of experience with regards to optimizing the shape of a neural network for next character prediction. There may be a way to optimize just a regular MLP for this task but the experience is lacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "What can I learn coming out of this?\n",
    "\n",
    "1. Why does sparsity affect the learning ability of a neural network?\n",
    "2. How does learning rate affect the learning ability of neural network? What happens when learning rate values become too large or small?\n",
    "3. How does a neural network learn in the first place? Can you learn to guestimate how well a given neural network will do for a specific dataset / task\n",
    "4. How does cross entropy loss work? Why does the formula use the exponent rather than simple absolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This current implementation learns to predict the most common character to follow an ngram containing any permutation of tokens in the bag.\n",
    "\n",
    "An effective way to confirm this would be to pick the tokens of the most common ngram in the dataset, then pull out all the permutations of that token and print their next token probabilities.\n",
    "\n",
    "Compare this then to the token probabilities calculated by the neural network. The probabilities should almost look like joint probabilities of the token permutations / ngrams in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "What about a 1-d convolutional neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
