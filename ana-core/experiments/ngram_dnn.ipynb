{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram BoW Deep Neural Network Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Scope\n",
    "--\n",
    "Learn to predict the next token in a sequences using a bag of words representation of an ngram\n",
    "\n",
    "Limitations\n",
    "--\n",
    "1. Character level tokenization will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "TRAIN_SIZE=0.8\n",
    "NGRAM_SIZE = 3\n",
    "BATCH_SIZE = 8\n",
    "NUM_BATCHES = 500\n",
    "N_EPOCHS = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @tiffanylue i know  i was listenin to bad habi...\n",
       "1    Layin n bed with a headache  ughhhh...waitin o...\n",
       "2                  Funeral ceremony...gloomy friday...\n",
       "3                 wants to hang out with friends SOON!\n",
       "4    @dannycastillo We want to trade with someone w...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/text/text_emotion.csv')['content']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 101 unique tokens\n"
     ]
    }
   ],
   "source": [
    "END_TOKEN = \"<E>\"\n",
    "\n",
    "vocab = [END_TOKEN] + list(set(dataset.str.cat(sep=' ')))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"The corpus contains {vocab_size} unique tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_by_token = {token: i for (i, token) in enumerate(vocab)}\n",
    "token_by_id = {id_: token for (token, id_) in id_by_token.items()}\n",
    "\n",
    "def encode(token: str) -> int:\n",
    "    return id_by_token[token]\n",
    "\n",
    "def decode(encoding: int) -> str:\n",
    "    return token_by_id[encoding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(document: str, is_complete: str = False) -> list[str]:\n",
    "    tokens = [END_TOKEN] + list(document) \n",
    "    if is_complete:\n",
    "        tokens.append(END_TOKEN)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_dataset = dataset[: math.floor(dataset_size * TRAIN_SIZE)]\n",
    "test_dataset = dataset[math.floor(dataset_size * TRAIN_SIZE):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options for building batches from the training set once we've defined our batch size\n",
    "\n",
    "1. Randomly sample a document from the training set and a starting index in the range \\[0, `document_length` - `batch_size`). Iterate over the document from `starting_index` to `starting_index` + `batch_size` - 1, building a training example at each iteration such that the training example is a 2-tuple of the ngram `document[max(starting_index, current_index - ngram_size + 1), curent_index + 1]` and the target is `document[current_index + 1]`. My only concern here is that with this approach we will get way fewer examples that contain the END_TOKEN.\n",
    "2. Iterate over each document in the training set, building training examples from indices in the range [0, `document_length` - 1). Each example will comprises the example: `document[max(starting_index, current_index - ngram_size + 1): current_index + 1]` and the target `document[current_index + 1]` \n",
    "\n",
    "We will go with option 1 initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_document(dataset, index: int = None) -> tuple[list, list[str]]:\n",
    "    \"\"\"Returns the document at the specified index in the provided dataset.\n",
    "    \n",
    "    If no index is specified then an index will be selected at random.\n",
    "    \"\"\"\n",
    "    if index is None:\n",
    "        index = random.randrange(0, dataset.shape[0])\n",
    "    return dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokens(seq: Sequence[str], count: int, start: int = None) -> str:\n",
    "    \"\"\"Returns a substring containing the specified number of tokens from `seq`.\n",
    "    \n",
    "    If `start` is not specified, the substring will start at an index selected at random, ensuring that the \n",
    "    resulting substring contains the number of tokens specified. \n",
    "    If `count` is -1 \n",
    "    If `start` is specified then the resulting substring will start from the character at index `start`. If \n",
    "    there are not enough tokens from the specified start index to the end of the document then a `ValueError`\n",
    "    will be raised. \n",
    "    \n",
    "    \"\"\"\n",
    "    if len(seq) < count:\n",
    "        raise ValueError(\"The provided sequence does not contain enough tokens to return the requested amount\") \n",
    "        \n",
    "    if count == -1:\n",
    "        start = 0\n",
    "    elif start is None:\n",
    "        start = random.randrange(0, len(seq) - count + 1)\n",
    "    \n",
    "        \n",
    "    if len(seq) - start < count:\n",
    "        # This may be unncessary if the starting index is randomly generated since we ensure that the required\n",
    "        # number of tokens is available but is a nice check regardless.\n",
    "        raise ValueError(\"The required number of tokens could not be extracted from the text given the\"\\\n",
    "                         \"starting index.\")\n",
    "      \n",
    "    return seq[start: start + count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_ngrams(tokens: list[str], ngram_size: int) -> list[list[str]]:\n",
    "    \"\"\"Returns ngrams from the specified document\"\"\"\n",
    "    return [tokens[max(0, i - ngram_size): i] for i in range(1, len(tokens) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_training_examples(tokens: list[str], ngram_size: int) -> list[tuple[list[str], str]]:\n",
    "    \"\"\"Creates training examples from a collection of tokens.\n",
    "    \n",
    "    Each training example is a tuple of between 1 and `ngram_size` tokens and the next token in the sequence.\n",
    "    \"\"\"\n",
    "    ngrams = to_ngrams(tokens, ngram_size=ngram_size)\n",
    "    ngrams = ngrams[0: len(ngrams) - 1]  # Truncate the last ngram\n",
    "    return [(ngrams[i], tokens[i+1]) for i in range(len(ngrams))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_training_examples(examples):\n",
    "    encoded_ngrams = torch.zeros(len(examples), vocab_size)\n",
    "    encoded_targets = torch.zeros(len(examples), vocab_size)\n",
    "    \n",
    "    for i, (ngram, target) in enumerate(examples):\n",
    "        for token in ngram:\n",
    "            encoded_ngrams[i, id_by_token[token]] += 1\n",
    "        encoded_targets[i, id_by_token[target]] += 1\n",
    "    return encoded_ngrams, encoded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_batches(\n",
    "    dataset: pd.Series, \n",
    "    batch_size: int,\n",
    "    num_batches: int,\n",
    "    ngram_size: int = 3, \n",
    "    document_index: int = None, \n",
    "    start_token_index: int = None\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    for i in range(num_batches):\n",
    "        # Keep retrieving random documents until one with the minimum number of tokens is found\n",
    "        # TODO: Add filtering for token count to `get_document`\n",
    "        while True:\n",
    "            document = get_document(dataset, index=document_index)\n",
    "            try:\n",
    "                tokens = get_tokens(tokenize(document), count=batch_size+1, start=start_token_index)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            break\n",
    "        examples = to_training_examples(tokens, ngram_size=ngram_size)\n",
    "        yield encode_training_examples(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 101])\n"
     ]
    }
   ],
   "source": [
    "for x, y in to_batches(train_dataset, batch_size=10, num_batches=1, ngram_size=3, document_index=0, start_token_index=0):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we will use a simple neural network, 5 layers deep as a test. Once the learning ability of this model is proven we can evaulate using a larger neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(vocab_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example using our untrained model. Here we are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next token is predicted to be: 'p'\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "# Set the input as the encoded start token. \n",
    "x = torch.zeros(vocab_size)\n",
    "x[id_by_token['<E>']] = 1\n",
    "# Predict the token that is most likely to follow the start token.\n",
    "logits = model(x.reshape(1, -1).to(device))\n",
    "proba = nn.Softmax(dim=1)(logits)\n",
    "y_pred = proba.argmax(1)\n",
    "print(f\"The next token is predicted to be: '{token_by_id[y_pred.item()]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.672632 [0 / 500000]\n",
      "loss: 3.405138 [10000 / 500000]\n",
      "loss: 3.225195 [20000 / 500000]\n",
      "loss: 3.288160 [30000 / 500000]\n",
      "loss: 2.754446 [40000 / 500000]\n",
      "loss: 2.990888 [50000 / 500000]\n",
      "loss: 3.720843 [60000 / 500000]\n",
      "loss: 3.031395 [70000 / 500000]\n",
      "loss: 3.262000 [80000 / 500000]\n",
      "loss: 3.281992 [90000 / 500000]\n",
      "loss: 3.229769 [100000 / 500000]\n",
      "loss: 3.375826 [110000 / 500000]\n",
      "loss: 2.854901 [120000 / 500000]\n",
      "loss: 2.582804 [130000 / 500000]\n",
      "loss: 2.922288 [140000 / 500000]\n",
      "loss: 2.906969 [150000 / 500000]\n",
      "loss: 3.373748 [160000 / 500000]\n",
      "loss: 2.827018 [170000 / 500000]\n",
      "loss: 2.990323 [180000 / 500000]\n",
      "loss: 2.622689 [190000 / 500000]\n",
      "loss: 2.613509 [200000 / 500000]\n",
      "loss: 4.422468 [210000 / 500000]\n",
      "loss: 2.605788 [220000 / 500000]\n",
      "loss: 3.104275 [230000 / 500000]\n",
      "loss: 2.926967 [240000 / 500000]\n",
      "loss: 2.951092 [250000 / 500000]\n",
      "loss: 3.128122 [260000 / 500000]\n",
      "loss: 3.639613 [270000 / 500000]\n",
      "loss: 2.734726 [280000 / 500000]\n",
      "loss: 2.281590 [290000 / 500000]\n",
      "loss: 2.505006 [300000 / 500000]\n",
      "loss: 2.700067 [310000 / 500000]\n",
      "loss: 2.413187 [320000 / 500000]\n",
      "loss: 2.562590 [330000 / 500000]\n",
      "loss: 2.738565 [340000 / 500000]\n",
      "loss: 2.963944 [350000 / 500000]\n",
      "loss: 2.559984 [360000 / 500000]\n",
      "loss: 3.228619 [370000 / 500000]\n",
      "loss: 2.635622 [380000 / 500000]\n",
      "loss: 2.374530 [390000 / 500000]\n",
      "loss: 3.254662 [400000 / 500000]\n",
      "loss: 4.741541 [410000 / 500000]\n",
      "loss: 3.130374 [420000 / 500000]\n",
      "loss: 2.420695 [430000 / 500000]\n",
      "loss: 2.808981 [440000 / 500000]\n",
      "loss: 3.769717 [450000 / 500000]\n",
      "loss: 2.207675 [460000 / 500000]\n",
      "loss: 2.800729 [470000 / 500000]\n",
      "loss: 2.469066 [480000 / 500000]\n",
      "loss: 2.301188 [490000 / 500000]\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "batch_size=10\n",
    "num_batches=500000\n",
    "ngram_size=3\n",
    "\n",
    "for batch_no, (x, y) in enumerate(to_batches(train_dataset, batch_size=batch_size, num_batches=num_batches, ngram_size=ngram_size)):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if batch_no % 10000 == 0:\n",
    "        print(f\"loss: {loss:>7f} [{batch_no} / {num_batches}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_batches = 500\n",
    "\n",
    "for batch in range(num_batches):\n",
    "    x, y = encode_training_batch(get_random_training_text())\n",
    "    if batch < 10:\n",
    "        print(x)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        correct = (y_pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    \n",
    "    \n",
    "print(f\"Test Accuracy: {((correct / num_batches * BATCH_SIZE)*100):>7f}, Avg loss[{loss} / {num_batches}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt: str = None):\n",
    "    if prompt is None:\n",
    "        prompt = decode(random.randint(0, vocab_size))\n",
    "        \n",
    "    while len(prompt) < 20:\n",
    "        tokens = tokenize(prompt, is_complete=False)\n",
    "        ngrams = to_ngrams(tokens, ngram_size=3)\n",
    "        last_ngram = ngrams[len(ngrams) - 1]\n",
    "        with torch.no_grad():\n",
    "            print(f\"Current ngram: {last_ngram}\")\n",
    "            encoded_ngram = encode_ngram(last_ngram, encode_fn=encode).to(device)\n",
    "            logits = model(encoded_ngram)\n",
    "            proba = nn.Softmax(dim=0)(logits)\n",
    "            top_predictions = proba.topk(5).indices.tolist()\n",
    "            print(\"Top 5 most likely next tokens:\")\n",
    "            for i, encoding in enumerate(top_predictions):\n",
    "                print(f'    #{i}. \"{decode(encoding)}\"')\n",
    "            next_token = decode(torch.multinomial(proba, num_samples=1)[0].item())\n",
    "            print(f'\"{next_token}\" was chosen')\n",
    "            prompt += next_token\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current ngram: ['<E>', 'H', 'i']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"t\"\n",
      "    #1. \"n\"\n",
      "    #2. \" \"\n",
      "    #3. \"s\"\n",
      "    #4. \"l\"\n",
      "\"u\" was chosen\n",
      "Current ngram: ['H', 'i', 'u']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"n\"\n",
      "    #1. \"t\"\n",
      "    #2. \"s\"\n",
      "    #3. \"l\"\n",
      "    #4. \"e\"\n",
      "\"t\" was chosen\n",
      "Current ngram: ['i', 'u', 't']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"n\"\n",
      "    #1. \"t\"\n",
      "    #2. \"s\"\n",
      "    #3. \"l\"\n",
      "    #4. \"e\"\n",
      "\" \" was chosen\n",
      "Current ngram: ['u', 't', ' ']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"t\"\n",
      "    #1. \"n\"\n",
      "    #2. \"h\"\n",
      "    #3. \"m\"\n",
      "    #4. \"s\"\n",
      "\"h\" was chosen\n",
      "Current ngram: ['t', ' ', 'h']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"e\"\n",
      "    #1. \"o\"\n",
      "    #2. \"a\"\n",
      "    #3. \"i\"\n",
      "    #4. \"h\"\n",
      "\"h\" was chosen\n",
      "Current ngram: [' ', 'h', 'h']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"e\"\n",
      "    #1. \"o\"\n",
      "    #2. \"a\"\n",
      "    #3. \"i\"\n",
      "    #4. \"h\"\n",
      "\"a\" was chosen\n",
      "Current ngram: ['h', 'h', 'a']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"e\"\n",
      "    #1. \"t\"\n",
      "    #2. \" \"\n",
      "    #3. \"n\"\n",
      "    #4. \"i\"\n",
      "\"n\" was chosen\n",
      "Current ngram: ['h', 'a', 'n']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \" \"\n",
      "    #1. \"n\"\n",
      "    #2. \"t\"\n",
      "    #3. \"s\"\n",
      "    #4. \"d\"\n",
      "\"n\" was chosen\n",
      "Current ngram: ['a', 'n', 'n']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \" \"\n",
      "    #1. \"t\"\n",
      "    #2. \"n\"\n",
      "    #3. \"s\"\n",
      "    #4. \"e\"\n",
      "\"s\" was chosen\n",
      "Current ngram: ['n', 'n', 's']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \" \"\n",
      "    #1. \"e\"\n",
      "    #2. \"i\"\n",
      "    #3. \"a\"\n",
      "    #4. \"t\"\n",
      "\" \" was chosen\n",
      "Current ngram: ['n', 's', ' ']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"o\"\n",
      "    #1. \"h\"\n",
      "    #2. \"a\"\n",
      "    #3. \"t\"\n",
      "    #4. \"i\"\n",
      "\"y\" was chosen\n",
      "Current ngram: ['s', ' ', 'y']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"o\"\n",
      "    #1. \"a\"\n",
      "    #2. \"h\"\n",
      "    #3. \"i\"\n",
      "    #4. \"t\"\n",
      "\"a\" was chosen\n",
      "Current ngram: [' ', 'y', 'a']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"t\"\n",
      "    #1. \"n\"\n",
      "    #2. \" \"\n",
      "    #3. \"s\"\n",
      "    #4. \"m\"\n",
      "\"d\" was chosen\n",
      "Current ngram: ['y', 'a', 'd']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \" \"\n",
      "    #1. \".\"\n",
      "    #2. \"!\"\n",
      "    #3. \"s\"\n",
      "    #4. \"e\"\n",
      "\".\" was chosen\n",
      "Current ngram: ['a', 'd', '.']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \" \"\n",
      "    #1. \".\"\n",
      "    #2. \"!\"\n",
      "    #3. \"<E>\"\n",
      "    #4. \"'\"\n",
      "\" \" was chosen\n",
      "Current ngram: ['d', '.', ' ']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \" \"\n",
      "    #1. \"o\"\n",
      "    #2. \"a\"\n",
      "    #3. \"i\"\n",
      "    #4. \"w\"\n",
      "\"'\" was chosen\n",
      "Current ngram: ['.', ' ', \"'\"]\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \" \"\n",
      "    #1. \"t\"\n",
      "    #2. \"m\"\n",
      "    #3. \"n\"\n",
      "    #4. \"w\"\n",
      "\"G\" was chosen\n",
      "Current ngram: [' ', \"'\", 'G']\n",
      "Top 5 most likely next tokens:\n",
      "    #0. \"t\"\n",
      "    #1. \"h\"\n",
      "    #2. \"o\"\n",
      "    #3. \"m\"\n",
      "    #4. \"w\"\n",
      "\" \" was chosen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hiut hhanns yad. 'G \""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: tensor([[0.0448, 0.0482, 0.0376, 0.0471, 0.0776, 0.1615, 0.1038, 0.2782, 0.0241,\n",
      "         0.1772]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "proba = softmax(torch.randn((1,10)))\n",
    "print(f\"Probabilities: {proba}\")\n",
    "torch.multinomial(proba, num_samples=1)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_ngram(ngram, encode_fn) -> torch.Tensor:\n",
    "    encoded_ngram = torch.zeros(vocab_size)\n",
    "    for token in ngram:\n",
    "        encoding = encode_fn(token)\n",
    "        encoded_ngram[encoding] += 1\n",
    "    return encoded_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joot  ot  ot  ot  ot'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('J')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
