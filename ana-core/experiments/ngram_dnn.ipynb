{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram BoW Deep Neural Network Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Scope\n",
    "--\n",
    "Learn to predict the next token in a sequences using a bag of words representation of an ngram\n",
    "\n",
    "Limitations\n",
    "--\n",
    "1. Character level tokenization will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "END_TOKEN = \"<E>\"\n",
    "NGRAM_SIZE = 3\n",
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 64\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author   \n",
       "0  1956967341       empty   xoshayzers  \\\n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/text/text_emotion.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = train_data['content'].str.cat(sep=' ')\n",
    "vocab = [END_TOKEN] + list(set(corpus))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_by_token = {token: i for (i, token) in enumerate(vocab)}\n",
    "token_by_id = {id_: token for (token, id_) in id_by_token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode(token: str) -> int:\n",
    "    return id_by_token[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode(encoding: int) -> str:\n",
    "    return token_by_id[encoding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(document: str) -> list[str]:\n",
    "    return list(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_document(idx: int = None) -> list[str]:\n",
    "    return ([END_TOKEN] + tokenize(train_data.loc[idx, 'content']) + [END_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_ngrams(tokens: list[str], max_ngram_size: int = 3) -> list[str]:\n",
    "    for i in range(1, len(tokens)):\n",
    "        yield tokens[max(i - max_ngram_size, 0): i]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training bataches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options for building batches from the training set once we've defined our batch size\n",
    "\n",
    "1. Randomly sample a document from the training set and a starting index in the range \\[0, `document_length` - `batch_size`). Iterate over the document from `starting_index` to `starting_index` + `batch_size` - 1, building a training example at each iteration such that the training example is a 2-tuple of the ngram `document[max(starting_index, current_index - ngram_size + 1), curent_index + 1]` and the target is `document[current_index + 1]`. My only concern here is that with this approach we will get way fewer examples that contain the END_TOKEN.\n",
    "2. Iterate over each document in the training set, building training examples from indices in the range [0, `document_length` - 1). Each example will comprises the example: `document[max(starting_index, current_index - ngram_size + 1): current_index + 1]` and the target `document[current_index + 1]` \n",
    "\n",
    "We will go with option 1 initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_training_text():\n",
    "    # Loop until we find a suitable training document\n",
    "    while True:\n",
    "        document_index = random.randrange(0, train_data.shape[0])\n",
    "        document = train_data.loc[document_index, 'content']\n",
    "        starting_index = random.randrange(0, len(document) - BATCH_SIZE)\n",
    "        example_text = document[starting_index: starting_index + BATCH_SIZE + 1]\n",
    "        if len(example_text) == BATCH_SIZE + 1:\n",
    "            break\n",
    "            \n",
    "    # Build the training examples\n",
    "    examples = []\n",
    "    for i in range(0, len(example_text) - 1):\n",
    "        ngram = example_text[max(0, i - NGRAM_SIZE + 1): i + 1]\n",
    "        next_token = example_text[i + 1]\n",
    "        examples.append((ngram, next_token))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_training_batch(batch):\n",
    "    encoded_ngrams = torch.zeros(BATCH_SIZE, vocab_size)\n",
    "    encoded_targets = torch.zeros(BATCH_SIZE, vocab_size)\n",
    "    \n",
    "    for i, (ngram, target) in enumerate(batch):\n",
    "        for token in ngram:\n",
    "            encoded_ngrams[i, id_by_token[token]] += 1\n",
    "        encoded_targets[i, id_by_token[target]] += 1\n",
    "        return encoded_ngrams, encoded_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Sampling Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 'a'),\n",
       " ('ra', 'g'),\n",
       " ('rag', 'e'),\n",
       " ('age', ' '),\n",
       " ('ge ', 'L'),\n",
       " ('e L', 'O'),\n",
       " (' LO', 'L'),\n",
       " ('LOL', ' ')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_training_text = get_random_training_text()\n",
    "random_training_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_training_batch(random_training_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can extract our training data and training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, train and evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we will use a simple neural network, 5 layers deep as a test. Once the learning ability of this model is proven we can evaulate using a larger neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(vocab_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example using our untrained model. Here we are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next token is predicted to be: '5'\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "# Set the input as the encoded start token. \n",
    "x = torch.zeros(vocab_size)\n",
    "x[id_by_token['<E>']] = 1\n",
    "# Predict the token that is most likely to follow the start token.\n",
    "logits = model(x.reshape(1, -1))\n",
    "proba = nn.Softmax(dim=1)(logits)\n",
    "y_pred = proba.argmax(1)\n",
    "print(f\"The next token is predicted to be: '{token_by_id[y_pred.item()]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_text = get_random_training_text()\n",
    "x, y = encode_training_batch(random_training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1106, -0.1442, -0.1683, -0.0626, -0.1253,  0.1609,  0.1735,  0.1489,\n",
       "          0.0523, -0.0943, -0.0430, -0.0118, -0.1190,  0.0565,  0.1071,  0.1095,\n",
       "          0.0617, -0.0849,  0.0220,  0.0395, -0.0099,  0.0710,  0.0425, -0.0972,\n",
       "          0.1173,  0.1017, -0.1083,  0.1051,  0.1330, -0.1324, -0.1272, -0.1011,\n",
       "          0.0283,  0.0401,  0.0003,  0.1219, -0.0661, -0.2128, -0.0577,  0.0026,\n",
       "         -0.1398,  0.0807,  0.0782, -0.0707, -0.1630, -0.1081, -0.0359,  0.1107,\n",
       "          0.2050, -0.1395,  0.0092,  0.1929, -0.1419,  0.0087,  0.0130, -0.0681,\n",
       "          0.0921, -0.0269, -0.0817,  0.1519, -0.1183,  0.1814,  0.2029, -0.0781,\n",
       "          0.1780, -0.0592,  0.0822,  0.1274,  0.0148, -0.0661, -0.1114, -0.1856,\n",
       "          0.1349, -0.0211,  0.1676, -0.0713, -0.1320,  0.0551,  0.1583, -0.1382,\n",
       "         -0.0604,  0.0409,  0.0278, -0.0417,  0.1207,  0.0370, -0.1308, -0.1313,\n",
       "          0.0123, -0.0273,  0.0869, -0.0702,  0.0017,  0.0587, -0.1159,  0.0574,\n",
       "         -0.0603,  0.1394, -0.1379,  0.1406,  0.2128],\n",
       "        [ 0.1084, -0.1442, -0.1670, -0.0630, -0.1227,  0.1607,  0.1740,  0.1480,\n",
       "          0.0535, -0.0933, -0.0465, -0.0113, -0.1183,  0.0555,  0.1078,  0.1085,\n",
       "          0.0635, -0.0862,  0.0252,  0.0393, -0.0107,  0.0689,  0.0424, -0.0958,\n",
       "          0.1190,  0.1012, -0.1092,  0.1052,  0.1324, -0.1310, -0.1265, -0.0995,\n",
       "          0.0291,  0.0399, -0.0003,  0.1208, -0.0668, -0.2120, -0.0556,  0.0052,\n",
       "         -0.1396,  0.0825,  0.0783, -0.0698, -0.1605, -0.1081, -0.0362,  0.1094,\n",
       "          0.2064, -0.1370,  0.0105,  0.1940, -0.1428,  0.0082,  0.0123, -0.0694,\n",
       "          0.0903, -0.0249, -0.0824,  0.1497, -0.1167,  0.1802,  0.2007, -0.0787,\n",
       "          0.1777, -0.0596,  0.0825,  0.1269,  0.0161, -0.0668, -0.1099, -0.1864,\n",
       "          0.1332, -0.0226,  0.1672, -0.0725, -0.1316,  0.0552,  0.1588, -0.1387,\n",
       "         -0.0618,  0.0412,  0.0287, -0.0418,  0.1213,  0.0387, -0.1328, -0.1311,\n",
       "          0.0103, -0.0282,  0.0843, -0.0691,  0.0012,  0.0573, -0.1175,  0.0583,\n",
       "         -0.0610,  0.1412, -0.1358,  0.1388,  0.2119],\n",
       "        [ 0.1084, -0.1442, -0.1670, -0.0630, -0.1227,  0.1607,  0.1740,  0.1480,\n",
       "          0.0535, -0.0933, -0.0465, -0.0113, -0.1183,  0.0555,  0.1078,  0.1085,\n",
       "          0.0635, -0.0862,  0.0252,  0.0393, -0.0107,  0.0689,  0.0424, -0.0958,\n",
       "          0.1190,  0.1012, -0.1092,  0.1052,  0.1324, -0.1310, -0.1265, -0.0995,\n",
       "          0.0291,  0.0399, -0.0003,  0.1208, -0.0668, -0.2120, -0.0556,  0.0052,\n",
       "         -0.1396,  0.0825,  0.0783, -0.0698, -0.1605, -0.1081, -0.0362,  0.1094,\n",
       "          0.2064, -0.1370,  0.0105,  0.1940, -0.1428,  0.0082,  0.0123, -0.0694,\n",
       "          0.0903, -0.0249, -0.0824,  0.1497, -0.1167,  0.1802,  0.2007, -0.0787,\n",
       "          0.1777, -0.0596,  0.0825,  0.1269,  0.0161, -0.0668, -0.1099, -0.1864,\n",
       "          0.1332, -0.0226,  0.1672, -0.0725, -0.1316,  0.0552,  0.1588, -0.1387,\n",
       "         -0.0618,  0.0412,  0.0287, -0.0418,  0.1213,  0.0387, -0.1328, -0.1311,\n",
       "          0.0103, -0.0282,  0.0843, -0.0691,  0.0012,  0.0573, -0.1175,  0.0583,\n",
       "         -0.0610,  0.1412, -0.1358,  0.1388,  0.2119],\n",
       "        [ 0.1084, -0.1442, -0.1670, -0.0630, -0.1227,  0.1607,  0.1740,  0.1480,\n",
       "          0.0535, -0.0933, -0.0465, -0.0113, -0.1183,  0.0555,  0.1078,  0.1085,\n",
       "          0.0635, -0.0862,  0.0252,  0.0393, -0.0107,  0.0689,  0.0424, -0.0958,\n",
       "          0.1190,  0.1012, -0.1092,  0.1052,  0.1324, -0.1310, -0.1265, -0.0995,\n",
       "          0.0291,  0.0399, -0.0003,  0.1208, -0.0668, -0.2120, -0.0556,  0.0052,\n",
       "         -0.1396,  0.0825,  0.0783, -0.0698, -0.1605, -0.1081, -0.0362,  0.1094,\n",
       "          0.2064, -0.1370,  0.0105,  0.1940, -0.1428,  0.0082,  0.0123, -0.0694,\n",
       "          0.0903, -0.0249, -0.0824,  0.1497, -0.1167,  0.1802,  0.2007, -0.0787,\n",
       "          0.1777, -0.0596,  0.0825,  0.1269,  0.0161, -0.0668, -0.1099, -0.1864,\n",
       "          0.1332, -0.0226,  0.1672, -0.0725, -0.1316,  0.0552,  0.1588, -0.1387,\n",
       "         -0.0618,  0.0412,  0.0287, -0.0418,  0.1213,  0.0387, -0.1328, -0.1311,\n",
       "          0.0103, -0.0282,  0.0843, -0.0691,  0.0012,  0.0573, -0.1175,  0.0583,\n",
       "         -0.0610,  0.1412, -0.1358,  0.1388,  0.2119],\n",
       "        [ 0.1084, -0.1442, -0.1670, -0.0630, -0.1227,  0.1607,  0.1740,  0.1480,\n",
       "          0.0535, -0.0933, -0.0465, -0.0113, -0.1183,  0.0555,  0.1078,  0.1085,\n",
       "          0.0635, -0.0862,  0.0252,  0.0393, -0.0107,  0.0689,  0.0424, -0.0958,\n",
       "          0.1190,  0.1012, -0.1092,  0.1052,  0.1324, -0.1310, -0.1265, -0.0995,\n",
       "          0.0291,  0.0399, -0.0003,  0.1208, -0.0668, -0.2120, -0.0556,  0.0052,\n",
       "         -0.1396,  0.0825,  0.0783, -0.0698, -0.1605, -0.1081, -0.0362,  0.1094,\n",
       "          0.2064, -0.1370,  0.0105,  0.1940, -0.1428,  0.0082,  0.0123, -0.0694,\n",
       "          0.0903, -0.0249, -0.0824,  0.1497, -0.1167,  0.1802,  0.2007, -0.0787,\n",
       "          0.1777, -0.0596,  0.0825,  0.1269,  0.0161, -0.0668, -0.1099, -0.1864,\n",
       "          0.1332, -0.0226,  0.1672, -0.0725, -0.1316,  0.0552,  0.1588, -0.1387,\n",
       "         -0.0618,  0.0412,  0.0287, -0.0418,  0.1213,  0.0387, -0.1328, -0.1311,\n",
       "          0.0103, -0.0282,  0.0843, -0.0691,  0.0012,  0.0573, -0.1175,  0.0583,\n",
       "         -0.0610,  0.1412, -0.1358,  0.1388,  0.2119],\n",
       "        [ 0.1084, -0.1442, -0.1670, -0.0630, -0.1227,  0.1607,  0.1740,  0.1480,\n",
       "          0.0535, -0.0933, -0.0465, -0.0113, -0.1183,  0.0555,  0.1078,  0.1085,\n",
       "          0.0635, -0.0862,  0.0252,  0.0393, -0.0107,  0.0689,  0.0424, -0.0958,\n",
       "          0.1190,  0.1012, -0.1092,  0.1052,  0.1324, -0.1310, -0.1265, -0.0995,\n",
       "          0.0291,  0.0399, -0.0003,  0.1208, -0.0668, -0.2120, -0.0556,  0.0052,\n",
       "         -0.1396,  0.0825,  0.0783, -0.0698, -0.1605, -0.1081, -0.0362,  0.1094,\n",
       "          0.2064, -0.1370,  0.0105,  0.1940, -0.1428,  0.0082,  0.0123, -0.0694,\n",
       "          0.0903, -0.0249, -0.0824,  0.1497, -0.1167,  0.1802,  0.2007, -0.0787,\n",
       "          0.1777, -0.0596,  0.0825,  0.1269,  0.0161, -0.0668, -0.1099, -0.1864,\n",
       "          0.1332, -0.0226,  0.1672, -0.0725, -0.1316,  0.0552,  0.1588, -0.1387,\n",
       "         -0.0618,  0.0412,  0.0287, -0.0418,  0.1213,  0.0387, -0.1328, -0.1311,\n",
       "          0.0103, -0.0282,  0.0843, -0.0691,  0.0012,  0.0573, -0.1175,  0.0583,\n",
       "         -0.0610,  0.1412, -0.1358,  0.1388,  0.2119],\n",
       "        [ 0.1084, -0.1442, -0.1670, -0.0630, -0.1227,  0.1607,  0.1740,  0.1480,\n",
       "          0.0535, -0.0933, -0.0465, -0.0113, -0.1183,  0.0555,  0.1078,  0.1085,\n",
       "          0.0635, -0.0862,  0.0252,  0.0393, -0.0107,  0.0689,  0.0424, -0.0958,\n",
       "          0.1190,  0.1012, -0.1092,  0.1052,  0.1324, -0.1310, -0.1265, -0.0995,\n",
       "          0.0291,  0.0399, -0.0003,  0.1208, -0.0668, -0.2120, -0.0556,  0.0052,\n",
       "         -0.1396,  0.0825,  0.0783, -0.0698, -0.1605, -0.1081, -0.0362,  0.1094,\n",
       "          0.2064, -0.1370,  0.0105,  0.1940, -0.1428,  0.0082,  0.0123, -0.0694,\n",
       "          0.0903, -0.0249, -0.0824,  0.1497, -0.1167,  0.1802,  0.2007, -0.0787,\n",
       "          0.1777, -0.0596,  0.0825,  0.1269,  0.0161, -0.0668, -0.1099, -0.1864,\n",
       "          0.1332, -0.0226,  0.1672, -0.0725, -0.1316,  0.0552,  0.1588, -0.1387,\n",
       "         -0.0618,  0.0412,  0.0287, -0.0418,  0.1213,  0.0387, -0.1328, -0.1311,\n",
       "          0.0103, -0.0282,  0.0843, -0.0691,  0.0012,  0.0573, -0.1175,  0.0583,\n",
       "         -0.0610,  0.1412, -0.1358,  0.1388,  0.2119],\n",
       "        [ 0.1084, -0.1442, -0.1670, -0.0630, -0.1227,  0.1607,  0.1740,  0.1480,\n",
       "          0.0535, -0.0933, -0.0465, -0.0113, -0.1183,  0.0555,  0.1078,  0.1085,\n",
       "          0.0635, -0.0862,  0.0252,  0.0393, -0.0107,  0.0689,  0.0424, -0.0958,\n",
       "          0.1190,  0.1012, -0.1092,  0.1052,  0.1324, -0.1310, -0.1265, -0.0995,\n",
       "          0.0291,  0.0399, -0.0003,  0.1208, -0.0668, -0.2120, -0.0556,  0.0052,\n",
       "         -0.1396,  0.0825,  0.0783, -0.0698, -0.1605, -0.1081, -0.0362,  0.1094,\n",
       "          0.2064, -0.1370,  0.0105,  0.1940, -0.1428,  0.0082,  0.0123, -0.0694,\n",
       "          0.0903, -0.0249, -0.0824,  0.1497, -0.1167,  0.1802,  0.2007, -0.0787,\n",
       "          0.1777, -0.0596,  0.0825,  0.1269,  0.0161, -0.0668, -0.1099, -0.1864,\n",
       "          0.1332, -0.0226,  0.1672, -0.0725, -0.1316,  0.0552,  0.1588, -0.1387,\n",
       "         -0.0618,  0.0412,  0.0287, -0.0418,  0.1213,  0.0387, -0.1328, -0.1311,\n",
       "          0.0103, -0.0282,  0.0843, -0.0691,  0.0012,  0.0573, -0.1175,  0.0583,\n",
       "         -0.0610,  0.1412, -0.1358,  0.1388,  0.2119]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "logits = model(x)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5713, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(logits, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
