{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram BoW Deep Neural Network Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Scope\n",
    "--\n",
    "Learn to predict the next token in a sequences using a bag of words representation of an ngram\n",
    "\n",
    "Limitations\n",
    "--\n",
    "1. Character level tokenization will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "TRAIN_SIZE=0.8\n",
    "NGRAM_SIZE = 3\n",
    "BATCH_SIZE = 8\n",
    "NUM_BATCHES = 500\n",
    "N_EPOCHS = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @tiffanylue i know  i was listenin to bad habi...\n",
       "1    Layin n bed with a headache  ughhhh...waitin o...\n",
       "2                  Funeral ceremony...gloomy friday...\n",
       "3                 wants to hang out with friends SOON!\n",
       "4    @dannycastillo We want to trade with someone w...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/text/text_emotion.csv')['content']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 101 unique tokens\n"
     ]
    }
   ],
   "source": [
    "END_TOKEN = \"<E>\"\n",
    "\n",
    "vocab = [END_TOKEN] + list(set(dataset.str.cat(sep=' ')))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"The corpus contains {vocab_size} unique tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_by_token = {token: i for (i, token) in enumerate(vocab)}\n",
    "token_by_id = {id_: token for (token, id_) in id_by_token.items()}\n",
    "\n",
    "def encode(token: str) -> int:\n",
    "    return id_by_token[token]\n",
    "\n",
    "def decode(encoding: int) -> str:\n",
    "    return token_by_id[encoding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(document: str) -> list[str]:\n",
    "    return [END_TOKEN] + list(document) + [END_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_dataset = dataset[: math.floor(dataset_size * TRAIN_SIZE)]\n",
    "test_dataset = dataset[math.floor(dataset_size * TRAIN_SIZE):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options for building batches from the training set once we've defined our batch size\n",
    "\n",
    "1. Randomly sample a document from the training set and a starting index in the range \\[0, `document_length` - `batch_size`). Iterate over the document from `starting_index` to `starting_index` + `batch_size` - 1, building a training example at each iteration such that the training example is a 2-tuple of the ngram `document[max(starting_index, current_index - ngram_size + 1), curent_index + 1]` and the target is `document[current_index + 1]`. My only concern here is that with this approach we will get way fewer examples that contain the END_TOKEN.\n",
    "2. Iterate over each document in the training set, building training examples from indices in the range [0, `document_length` - 1). Each example will comprises the example: `document[max(starting_index, current_index - ngram_size + 1): current_index + 1]` and the target `document[current_index + 1]` \n",
    "\n",
    "We will go with option 1 initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_document(dataset, idx: int = None) -> tuple[list, list[str]]:\n",
    "    if idx is not None:\n",
    "        # Select the document at the specified index.\n",
    "        document = dataset[idx]\n",
    "    else:\n",
    "        # Or loop until we find a suitable training document.\n",
    "        while True:\n",
    "            document_index = random.randrange(0, dataset.shape[0])\n",
    "            document = dataset[document_index]\n",
    "            if len(document) >= BATCH_SIZE + 1: \n",
    "                break\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokenized_snippet(text: str, num_tokens: int, start: int = None) -> str:\n",
    "    \"\"\"Returns a substring containing the specified number of tokens from `text`.\n",
    "    \n",
    "    If `start` is specified then the resulting substring will start from the character at index `start`. If \n",
    "    there are not enough tokens from the specified start index to the end of the document then a `ValueError`\n",
    "    will be raised. \n",
    "    If `start` is not specified, the substring will start at an index selected at random, ensuring that the \n",
    "    resulting substring contains the number of tokens specified. \n",
    "    \"\"\"\n",
    "    tokens = None\n",
    "    if start is not None:\n",
    "        if len(text) - start_index < num_tokens:\n",
    "            raise ValueError(\"The required number of tokens could not be extracted from the text using the\"\\\n",
    "                             \"provided starting index\")\n",
    "        tokens = text[start: start + num_tokens]\n",
    "    else:\n",
    "        starting_index = random.randrange(0, len(tokenized_document) - BATCH_SIZE)\n",
    "        example_substr = tokenized_document[starting_index: starting_index + BATCH_SIZE + 1]\n",
    "    return (document, example_substr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@fobchick08 You lucky girl. Tell me all about it, 'kay?\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = get_document(train_dataset)\n",
    "tokenized_document = tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_ngrams(tokens: list[str], max_ngram_size: int = 3) -> list[str]:\n",
    "    for i in range(1, len(tokens)):\n",
    "        yield tokens[max(i - max_ngram_size, 0): i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[',\n",
       " ['e', 'r', ' ', 'a', 'n', 'd', ' ', 'i', ' '])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_document(train_dataset, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['r'],\n",
       " ['r', 'e'],\n",
       " ['r', 'e', ' '],\n",
       " ['e', ' ', 's'],\n",
       " [' ', 's', 'u'],\n",
       " ['s', 'u', 'p'],\n",
       " ['u', 'p', 'e'],\n",
       " ['p', 'e', 'r']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(to_ngrams(get_random_training_examples(train_dataset)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_training_examples(dataset):\n",
    "    \n",
    "            \n",
    "    # Build the training examples\n",
    "    examples = []\n",
    "    for i in range(0, len(example_text) - 1):\n",
    "        ngram = example_text[max(0, i - NGRAM_SIZE + 1): i + 1]\n",
    "        next_token = example_text[i + 1]\n",
    "        examples.append((ngram, next_token))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_training_batch(batch):\n",
    "    encoded_ngrams = torch.zeros(BATCH_SIZE, vocab_size)\n",
    "    encoded_targets = torch.zeros(BATCH_SIZE, vocab_size)\n",
    "    \n",
    "    for i, (ngram, target) in enumerate(batch):\n",
    "        for token in ngram:\n",
    "            encoded_ngrams[i, id_by_token[token]] += 1\n",
    "        encoded_targets[i, id_by_token[target]] += 1\n",
    "        return encoded_ngrams, encoded_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Sampling Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_training_text = get_random_training_text()\n",
    "random_training_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encode_training_batch(random_training_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can extract our training data and training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, train and evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we will use a simple neural network, 5 layers deep as a test. Once the learning ability of this model is proven we can evaulate using a larger neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(vocab_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example using our untrained model. Here we are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "# Set the input as the encoded start token. \n",
    "x = torch.zeros(vocab_size)\n",
    "x[id_by_token['<E>']] = 1\n",
    "# Predict the token that is most likely to follow the start token.\n",
    "logits = model(x.reshape(1, -1))\n",
    "proba = nn.Softmax(dim=1)(logits)\n",
    "y_pred = proba.argmax(1)\n",
    "print(f\"The next token is predicted to be: '{token_by_id[y_pred.item()]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_text = get_random_training_text()\n",
    "x, y = encode_training_batch(random_training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_batches = 5000\n",
    "\n",
    "for batch in range(num_batches):\n",
    "    x, y = encode_training_batch(get_random_training_text())\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "        print(f\"loss: {loss:>7f} [{batch} / {num_batches}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_batches = 500\n",
    "\n",
    "for batch in range(num_batches):\n",
    "    x, y = encode_training_batch(get_random_training_text())\n",
    "    if batch < 10:\n",
    "        print(x)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        correct = (y_pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    \n",
    "    \n",
    "print(f\"Test Accuracy: {((correct / num_batches * BATCH_SIZE)*100):>7f}, Avg loss[{loss} / {num_batches}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
