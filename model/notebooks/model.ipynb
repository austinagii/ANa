{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f361596-1061-4efb-8dba-26c16d4c881a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging \n",
    "import itertools\n",
    "import random\n",
    "import torch \n",
    "import torch.random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from pathlib import Path \n",
    "from typing import Union, Generator\n",
    "from collections.abc import Iterable\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    format='[%(levelname)s - %(asctime)s] %(message)s',\n",
    "                    datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "DATASET_PATH = \"../data/text_emotion.csv\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "NGRAM_SIZE = 4\n",
    "MAX_RESPONSE_LENGTH=100\n",
    "SENTENCE_SEPARATOR = \". \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810e5ff-df75-45de-978f-9bcd4d223454",
   "metadata": {},
   "source": [
    "# Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b9436e-d430-41cb-9278-550a181b99bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO - 04/10/2023 09:02:15 AM] Loading dataset from '/Users/kadeem/Spaces/Projects/ANa/ana-core/data/text_emotion.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the head of the dataset or throw an error if the dataset file could not be found\n",
    "dataset_path = Path(DATASET_PATH).resolve()\n",
    "if not dataset_path.exists() or not dataset_path.is_file():\n",
    "    logging.error(f'The dataset file could not be found at the specified path: \\'{dataset_path}\\'')\n",
    "    raise FileNotFoundError('The dataset file could not be found at the specified path')\n",
    "logging.info(f'Loading dataset from \\'{dataset_path}\\'')\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c93ea5-136c-49d2-9923-9e11af2ec278",
   "metadata": {},
   "source": [
    "# Create a text corpus from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebaf4ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of corpus: '@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[. Layin '\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset['content'].str.cat(sep=SENTENCE_SEPARATOR)\n",
    "print(f\"Sample of corpus: '{corpus[:100]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3092591-b06a-49d3-a881-6c7796889fad",
   "metadata": {},
   "source": [
    "# Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a73c0f4-0af7-4ba0-a485-187b21a58b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> list[str]:\n",
    "        \"\"\"Returns the input text as a sequence of tokens\n",
    "\n",
    "        The input is tokenized at character level and returns each character\n",
    "        in the order they appear in the input\n",
    "        \"\"\"\n",
    "        return (char for char in text) if text is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e00898-cd25-48e0-8e94-b731eeb55f70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of training tokens: ['@', 't', 'i', 'f', 'f', 'a', 'n', 'y', 'l', 'u']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample of training tokens: {list(itertools.islice(Tokenizer.tokenize(corpus), 10))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb7a26-c195-43f1-ac41-a0a1a7670638",
   "metadata": {},
   "source": [
    "# Create the token codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0fa4cde-9888-4e29-b416-94ebc55b1c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenCodec:\n",
    "    def __init__(self, token_stream: Generator[str, None, None]):\n",
    "        # get the unique tokens in the token stream\n",
    "        unique_tokens = set()\n",
    "        for token in token_stream:\n",
    "            unique_tokens.add(token)\n",
    "        self.alphabet_size = len(unique_tokens)\n",
    "        # map each unique token to an id\n",
    "        self._id_by_token = {token: _id for _id, token in enumerate(unique_tokens)}\n",
    "        # create the reverse mapping from ids to tokens\n",
    "        self._token_by_id = {_id: token for token, _id in self._id_by_token.items()}\n",
    "    \n",
    "    def encode(self, text: Union[str, Iterable[str]]) -> list[int]:\n",
    "        return [self._id_by_token[token] for token in text]\n",
    "    \n",
    "    def decode(self, encoded_text: Iterable[int]) -> str:\n",
    "        return \"\".join([self._token_by_id[_id] for _id in encoded_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41298656-7578-43fb-9a2e-08c86430afae",
   "metadata": {},
   "source": [
    "# Create the ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa9bbf8-84e8-45c4-b2ce-9058442a7bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_ngrams(corpus, tokenizer, ngram_size=NGRAM_SIZE):\n",
    "    token_sequences = [itertools.islice(Tokenizer.tokenize(corpus), pos, None) for pos in range(ngram_size)]\n",
    "    assert len(token_sequences) == ngram_size\n",
    "    while True:\n",
    "        try:\n",
    "            ngram = [next(seq) for seq in token_sequences]\n",
    "            yield ngram\n",
    "        except StopIteration:\n",
    "            break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78b8fc48-f6a3-44cf-b4ee-2dedd4bf9d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: @tiffanylu\n",
      "n-grams:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['@', 't', 'i', 'f'],\n",
       " ['t', 'i', 'f', 'f'],\n",
       " ['i', 'f', 'f', 'a'],\n",
       " ['f', 'f', 'a', 'n'],\n",
       " ['f', 'a', 'n', 'y'],\n",
       " ['a', 'n', 'y', 'l'],\n",
       " ['n', 'y', 'l', 'u']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample = corpus[:10]\n",
    "print(f\"Text sample: {corpus_sample}\")\n",
    "print(\"n-grams:\")\n",
    "list(to_ngrams(corpus_sample, Tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62bd36b9-6c9e-46b8-9d59-35db83ee2f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def __init__(self, codec: TokenCodec, ngram_size:int=NGRAM_SIZE, max_response_length=MAX_RESPONSE_LENGTH):\n",
    "        logging.info(f\"Initializing a {ngram_size}-gram model\")\n",
    "        self.codec = codec\n",
    "        self.ngram_size = ngram_size\n",
    "        self.max_response_length = max_response_length\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"Train the model on the tokens from the training corpus\"\"\"\n",
    "        # count the occurrences of each ngram\n",
    "        self._ngram_frequencies = torch.zeros(self.ngram_size * [self.codec.alphabet_size], dtype=torch.int32)\n",
    "        for ngram in to_ngrams(corpus, Tokenizer, ngram_size=self.ngram_size):\n",
    "            encoded_ngram = tuple(self.codec.encode(ngram))\n",
    "            self._ngram_frequencies[encoded_ngram] = self._ngram_frequencies[encoded_ngram].item() + 1\n",
    "        # build a probability distribution of each ngram based on their frequency\n",
    "        self._ngram_proba = self._ngram_frequencies / self._ngram_frequencies.sum(axis=self.ngram_size - 1, keepdims=True)\n",
    "        return self\n",
    "    \n",
    "    def __call__(self, prompt=None):\n",
    "        # TODO: handle the case where the prompt is shorter than the ngram size\n",
    "        # choose a random letter as the starting prompt is none is provided\n",
    "        if prompt is None or prompt == \"\":\n",
    "            # prompt = self._vocabulary[random.randint(0, len(self._vocabulary))]\n",
    "            prompt = \"dog\"\n",
    "        # initialize the response with the prompt as the lead \n",
    "        response = codec.encode(prompt)\n",
    "        # build the remainder of the response one token at a time using the ngram model\n",
    "        next_token = None\n",
    "        count = 0\n",
    "        while next_token != self.codec.encode('.')[0] and count < self.max_response_length: \n",
    "        # for i in range(self.max_response_length):\n",
    "            leading_tokens = tuple(response[-self.ngram_size+1:])\n",
    "            next_token = torch.multinomial(self._ngram_proba[leading_tokens], num_samples=1).item()\n",
    "            response.append(next_token)\n",
    "            count += 1\n",
    "        return self.codec.decode(response)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4027fef1-fe40-49c0-8c03-697cb327cb6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO - 04/10/2023 09:02:15 AM] Initializing a 4-gram model\n"
     ]
    }
   ],
   "source": [
    "codec = TokenCodec(Tokenizer.tokenize(corpus))\n",
    "model = NGramModel(codec, max_response_length=50).train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b2605b-c7ee-4be9-93ae-54b47c5d37db",
   "metadata": {},
   "source": [
    "# Generate a few responses using the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8577bfd8-8e43-4c94-b6a5-0270708b151f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses from the prompt 'Today':\n",
      "  Today new u remer cool press backs to sepin whold!. [51 chars]\n",
      "  Today oftw. [11 chars]\n",
      "  Today. [6 chars]\n",
      "  Today like the flately traces in 2 getty Oh, I littere  [55 chars]\n",
      "  Today ule anday. [16 chars]\n",
      "  Todays there take stuffice is hat one he globby_rence n [55 chars]\n",
      "  Today. [6 chars]\n",
      "  Today havey and reat!?!? Reebodi Hi btw. [40 chars]\n",
      "  Today get a why. [16 chars]\n",
      "  Today moviewedding like conder 18 One o2 - is time!. [52 chars]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Today'\n",
    "# Generate a few responses from the same prompt\n",
    "print(f\"Responses from the prompt '{prompt}':\")\n",
    "for i in range(10):\n",
    "    response = model(prompt)\n",
    "    print(f'  {response} [{len(response)} chars]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5d2e4-2e57-457c-afee-5cf96dbab3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
